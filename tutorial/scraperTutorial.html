<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=cGvuclDC_Z1vE_cnVEU6Ae_NZQ7StBcqH_vXVqoPMX0');.lst-kix_egkpyywq84m4-4>li:before{content:"\0025cb  "}.lst-kix_egkpyywq84m4-3>li:before{content:"\0025cf  "}.lst-kix_egkpyywq84m4-5>li:before{content:"\0025a0  "}.lst-kix_8fuq8mrjccaa-0>li:before{content:"\0025cf  "}.lst-kix_8fuq8mrjccaa-2>li:before{content:"\0025a0  "}.lst-kix_egkpyywq84m4-2>li:before{content:"\0025a0  "}.lst-kix_egkpyywq84m4-6>li:before{content:"\0025cf  "}.lst-kix_avobg7ugvjvy-0>li:before{content:"\0025cf  "}.lst-kix_8fuq8mrjccaa-3>li:before{content:"\0025cf  "}.lst-kix_avobg7ugvjvy-1>li:before{content:"\0025cb  "}.lst-kix_avobg7ugvjvy-2>li:before{content:"\0025a0  "}.lst-kix_ryua6xjiznqm-8>li:before{content:"-  "}.lst-kix_8fuq8mrjccaa-1>li:before{content:"\0025cb  "}.lst-kix_ryua6xjiznqm-5>li:before{content:"-  "}ul.lst-kix_avobg7ugvjvy-6{list-style-type:none}ul.lst-kix_6xyoogcg4v-8{list-style-type:none}ul.lst-kix_avobg7ugvjvy-7{list-style-type:none}ul.lst-kix_6xyoogcg4v-7{list-style-type:none}ul.lst-kix_avobg7ugvjvy-4{list-style-type:none}ul.lst-kix_avobg7ugvjvy-5{list-style-type:none}.lst-kix_8fuq8mrjccaa-8>li:before{content:"\0025a0  "}.lst-kix_ryua6xjiznqm-3>li:before{content:"-  "}.lst-kix_ryua6xjiznqm-7>li:before{content:"-  "}ul.lst-kix_avobg7ugvjvy-2{list-style-type:none}ul.lst-kix_6xyoogcg4v-4{list-style-type:none}ul.lst-kix_avobg7ugvjvy-3{list-style-type:none}ul.lst-kix_6xyoogcg4v-3{list-style-type:none}.lst-kix_ryua6xjiznqm-2>li:before{content:"-  "}.lst-kix_ryua6xjiznqm-6>li:before{content:"-  "}ul.lst-kix_avobg7ugvjvy-0{list-style-type:none}ul.lst-kix_6xyoogcg4v-6{list-style-type:none}ul.lst-kix_avobg7ugvjvy-1{list-style-type:none}.lst-kix_8fuq8mrjccaa-7>li:before{content:"\0025cb  "}ul.lst-kix_6xyoogcg4v-5{list-style-type:none}.lst-kix_egkpyywq84m4-0>li:before{content:"\0025cf  "}ul.lst-kix_6xyoogcg4v-0{list-style-type:none}.lst-kix_egkpyywq84m4-1>li:before{content:"\0025cb  "}ul.lst-kix_6xyoogcg4v-2{list-style-type:none}.lst-kix_8fuq8mrjccaa-4>li:before{content:"\0025cb  "}.lst-kix_8fuq8mrjccaa-6>li:before{content:"\0025cf  "}ul.lst-kix_6xyoogcg4v-1{list-style-type:none}.lst-kix_ryua6xjiznqm-4>li:before{content:"-  "}ul.lst-kix_avobg7ugvjvy-8{list-style-type:none}.lst-kix_8fuq8mrjccaa-5>li:before{content:"\0025a0  "}ul.lst-kix_8fuq8mrjccaa-5{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-4{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-3{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-2{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-8{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-7{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-6{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-1{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-0{list-style-type:none}ul.lst-kix_ryua6xjiznqm-6{list-style-type:none}.lst-kix_6xyoogcg4v-3>li:before{content:"\0025cf  "}ul.lst-kix_ryua6xjiznqm-5{list-style-type:none}ul.lst-kix_ryua6xjiznqm-8{list-style-type:none}ul.lst-kix_ryua6xjiznqm-7{list-style-type:none}.lst-kix_6xyoogcg4v-1>li:before{content:"\0025cb  "}.lst-kix_6xyoogcg4v-5>li:before{content:"\0025a0  "}.lst-kix_6xyoogcg4v-0>li:before{content:"\0025cf  "}.lst-kix_6xyoogcg4v-4>li:before{content:"\0025cb  "}.lst-kix_egkpyywq84m4-8>li:before{content:"\0025a0  "}.lst-kix_egkpyywq84m4-7>li:before{content:"\0025cb  "}ul.lst-kix_ryua6xjiznqm-0{list-style-type:none}ul.lst-kix_ryua6xjiznqm-2{list-style-type:none}ul.lst-kix_ryua6xjiznqm-1{list-style-type:none}ul.lst-kix_ryua6xjiznqm-4{list-style-type:none}.lst-kix_6xyoogcg4v-2>li:before{content:"\0025a0  "}ul.lst-kix_ryua6xjiznqm-3{list-style-type:none}ul.lst-kix_egkpyywq84m4-0{list-style-type:none}ul.lst-kix_egkpyywq84m4-1{list-style-type:none}ul.lst-kix_egkpyywq84m4-2{list-style-type:none}ul.lst-kix_egkpyywq84m4-3{list-style-type:none}.lst-kix_6xyoogcg4v-8>li:before{content:"\0025a0  "}ul.lst-kix_egkpyywq84m4-4{list-style-type:none}ul.lst-kix_egkpyywq84m4-5{list-style-type:none}.lst-kix_6xyoogcg4v-7>li:before{content:"\0025cb  "}ul.lst-kix_egkpyywq84m4-6{list-style-type:none}ul.lst-kix_egkpyywq84m4-7{list-style-type:none}ul.lst-kix_egkpyywq84m4-8{list-style-type:none}.lst-kix_6xyoogcg4v-6>li:before{content:"\0025cf  "}.lst-kix_ryua6xjiznqm-1>li:before{content:"-  "}.lst-kix_avobg7ugvjvy-8>li:before{content:"\0025a0  "}.lst-kix_ryua6xjiznqm-0>li:before{content:"-  "}.lst-kix_avobg7ugvjvy-4>li:before{content:"\0025cb  "}.lst-kix_avobg7ugvjvy-5>li:before{content:"\0025a0  "}.lst-kix_avobg7ugvjvy-3>li:before{content:"\0025cf  "}.lst-kix_avobg7ugvjvy-7>li:before{content:"\0025cb  "}.lst-kix_avobg7ugvjvy-6>li:before{content:"\0025cf  "}ol{margin:0;padding:0}table td,table th{padding:0}.c20{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:468pt;border-top-color:#000000;border-bottom-style:solid}.c13{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:504.8pt;border-top-color:#000000;border-bottom-style:solid}.c25{margin-left:36pt;padding-top:0pt;text-indent:36pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:right}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c3{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c15{border-spacing:0;border-collapse:collapse;margin-right:auto}.c9{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c11{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c23{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c18{font-weight:400;font-size:12pt;font-family:"Arial"}.c2{font-size:10pt;font-family:"Consolas";font-weight:400}.c12{font-weight:700;font-size:12pt;font-family:"Arial"}.c21{font-weight:700;font-size:14pt;font-family:"Arial"}.c10{margin-left:36pt;padding-left:0pt}.c16{padding:0;margin:0}.c1{color:inherit;text-decoration:inherit}.c0{font-weight:700;font-family:"Consolas"}.c24{margin-left:72pt;padding-left:0pt}.c7{font-weight:400;font-family:"Consolas"}.c22{font-size:12pt;font-weight:700}.c26{font-size:11pt}.c19{height:11pt}.c4{font-size:10pt}.c17{height:0pt}.c14{font-style:italic}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c23"><div><p class="c25"><span class="c6">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Edited 9/30/19</span></p></div><p class="c8"><span class="c9 c21">Chi Hack Night Scraper Tutorial</span></p><p class="c8"><span>In this tutorial, we will be creating a 2 web scrapers for the Chi Hack Night events page </span><span class="c3"><a class="c1" href="https://www.google.com/url?q=https://chihacknight.org/events/index.html&amp;sa=D&amp;ust=1570341218898000">https://chihacknight.org/events/index.html</a></span><span class="c6">. The first one will be simple and only extract data from the event listing page. The second one will be more complex and extract more complete data from each event page.</span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span>If you want to reference the completed scrapers, the repository already contains both web scrapers you will create in this tutorial (</span><span class="c14">Authors note: not yet actually (TODO)</span><span class="c6">)</span></p><p class="c8"><span class="c9 c7 c26">event_processor/scrapers/chihacknight_simple_spider.py</span></p><p class="c8"><span class="c9 c7 c26">event_processor/scrapers/chihacknight_crawling_spider.py</span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c9 c21">Prerequisites</span></p><p class="c8"><span class="c6">You should be comfortable with: </span></p><ul class="c16 lst-kix_8fuq8mrjccaa-0 start"><li class="c8 c10"><span class="c6">HTML/DOM</span></li><li class="c8 c10"><span class="c6">CSS Selectors</span></li><li class="c8 c10"><span class="c6">Python</span></li></ul><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c6">This tutorial assumes you are using a Linux machine, so if you&rsquo;re using something else the shell commands would be a little different. </span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c9 c21">Web Scrapers in In2It</span></p><p class="c8"><span class="c6">Web scrapers are important for the the In2It Chicago project because they extract all event data for the site in a relatively normalized and searchable format. While users could always use a traditional search engine to find these events, In2It Chicago is meant to focus on events related to community involvement and support. Each web scraper is tailor made for each site.</span></p><p class="c8"><span>The scrapers are python classes which inherit classes from scrapy </span><span class="c3"><a class="c1" href="https://www.google.com/url?q=https://scrapy.org/&amp;sa=D&amp;ust=1570341218901000">https://scrapy.org/</a></span><span class="c6">, which is the most popular python web scraping library available.</span></p><p class="c8"><span class="c6">Other than web scrapers, the site also extracts data from APIs (and what else)? </span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c9 c21">Project file structure</span></p><p class="c8"><span>The web scrapers all live in the </span><span class="c7">event_processor</span><span class="c6">&nbsp;container- it is just one of the many In2It Chicago docker containers. </span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span>The subfolder </span><span class="c7">event_processor/event_processor</span><span class="c6">&nbsp;is a python package containing all relevant files. The important subfolders are as follows:</span></p><ul class="c16 lst-kix_avobg7ugvjvy-0 start"><li class="c8 c10"><span class="c7">base</span><span class="c6">&nbsp;- All base classes for custom data extractors</span></li><li class="c8 c10"><span class="c7">scrapers</span><span class="c6">&nbsp;- Custom web scrapers</span></li><li class="c8 c10"><span class="c7">apis</span><span class="c6">&nbsp;- Classes extracting data from public APIs </span></li><li class="c8 c10"><span class="c7">scrapy_impl</span><span class="c6">&nbsp;- Scrapy middleware </span></li><li class="c8 c10"><span class="c7">models</span><span class="c6">&nbsp;- Classes and utility functions for data container classes</span></li><li class="c8 c10"><span class="c7">utils</span><span class="c6">&nbsp;- Utility functions</span></li></ul><p class="c5"><span class="c6"></span></p><p class="c8"><span>Obviously, the folder we care about most in this tutorial is the </span><span class="c7">scrapers</span><span class="c6">&nbsp;folder. The first section will be for a simple scraper that only scrapes one page. The second section is a scraper that crawls and scrapes many pages. </span></p><p class="c5"><span class="c6"></span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c9 c21">Simple Web Scraper</span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span>The more simple ScraperSpider inherits from scrapy&rsquo;s ScrapySpider </span><span class="c3"><a class="c1" href="https://www.google.com/url?q=https://docs.scrapy.org/en/latest/topics/spiders.html%23scrapy-spider&amp;sa=D&amp;ust=1570341218904000">https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy-spider</a></span><span class="c6">&nbsp;and includes other utility methods specific to this project. </span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c9 c12">Define the Scraper</span></p><p class="c8"><span>Go ahead and create a new custom scraper file in </span><span class="c7">event_processor/scrapers</span><span>&nbsp;called </span><span class="c0">chihacknight_spider.py</span><span class="c6">.</span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span>Import the necessary base class for the simple scraper, ScraperSpider and define the spider name. The </span><span class="c7">coding: utf-8</span><span>&nbsp;comment on top defines the character encoding of the file. (</span><span class="c14">Authors note: I don&rsquo;t know if that is what it does or if it is even needed in python</span><span class="c6">) </span></p><p class="c5"><span class="c6"></span></p><a id="t.7640930352fb9d44052614f0946ddc944ff8ceef"></a><a id="t.0"></a><table class="c15"><tbody><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c11"><span class="c9 c2"># -*- coding: utf-8 -*-</span></p><p class="c11"><span class="c0 c4">from</span><span class="c2">&nbsp;event_processor.base.custom_spiders </span><span class="c0 c4">import</span><span class="c9 c2">&nbsp;ScraperSpider</span></p><p class="c11 c19"><span class="c9 c2"></span></p><p class="c11"><span class="c0 c4">class</span><span class="c9 c2">&nbsp;ChiHackNightSpider(ScraperSpider):</span></p><p class="c11"><span class="c9 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name = &#39;chihacknight&#39;</span></p><p class="c11"><span class="c9 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;allowed_domains = [&#39;chihacknight.org&#39;]</span></p><p class="c11"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enabled = True</span></p></td></tr></tbody></table><p class="c5"><span class="c6"></span></p><p class="c8"><span>The </span><span class="c0">name</span><span class="c6">&nbsp;attribute is how the scraper will be identified when running this spider from the command line. </span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span>The </span><span class="c0">allowed_domains</span><span>&nbsp;attribute specifies what domains this scraper will be able to visit while it is scraping (</span><span class="c14">Authors note: I don&rsquo;t know that for sure I&rsquo;m just assuming</span><span>). Because we want to scrape the Chi Hack Night website, we only specify the </span><span class="c7">&lsquo;chihacknight.org&rsquo;</span><span class="c6">&nbsp;domain. </span></p><p class="c8"><span>While the </span><span class="c0">enabled</span><span>&nbsp;attribute is not required, it is useful to know that you can set enabled to </span><span class="c7">False</span><span class="c6">&nbsp;to stop the event_processor from running or scheduling this scraper. </span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c9 c12">Initialize the Scraper</span></p><p class="c8"><span>We have to specify the parameters for the scraper and tell it where it where exactly it will be scraping data. Define the the </span><span class="c7">__init__</span><span>&nbsp;and </span><span class="c7">start_requests</span><span class="c6">&nbsp;methods for these purposes.</span></p><p class="c5"><span class="c6"></span></p><a id="t.109969e0d51672745ecb2d44c176631a6b5606a8"></a><a id="t.1"></a><table class="c15"><tbody><tr class="c17"><td class="c20" colspan="1" rowspan="1"><p class="c11"><span class="c0 c4">def </span><span class="c2">__init__(self, name=None, </span><span class="c0 c4">**</span><span class="c9 c2">kwargs):</span></p><p class="c11"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0 c4">super</span><span class="c9 c2">().__init__(self, &#39;Chi Hack Night&#39;, &#39;https://chihacknight.org/&#39;, date_format=&#39;%b %d, %Y&#39;, **kwargs)</span></p><p class="c11"><span class="c0 c4">def </span><span class="c9 c2">start_requests(self):</span></p><p class="c11"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c0 c4">yield </span><span class="c9 c2">self.get_request(&#39;events/&#39;, {})</span></p></td></tr></tbody></table><p class="c5"><span class="c6"></span></p><p class="c8"><span>The date_format is a required argument on __init__ and it is a string specifying the date format the scraper will be searching for. The string must follow the format expected by python&rsquo;s </span><span class="c7">datetime.strptime()</span><span>&nbsp;function. A full reference can be found at </span><span class="c3"><a class="c1" href="https://www.google.com/url?q=https://docs.python.org/3/library/datetime.html%23strftime-strptime-behavior&amp;sa=D&amp;ust=1570341218910000">https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior</a></span><span>. Manually examining the event format, it looks like </span><span class="c7">&lsquo;%b %d, %Y&rsquo;</span><span class="c6">&nbsp;</span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span>The </span><span class="c7">start_requests()</span><span>&nbsp;method specifies the page which the requests will start from. For spiders that extend from the </span><span class="c7">ScraperSpider</span><span class="c6">&nbsp;base class. </span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c22">Parse the Results</span></p><p class="c8"><span>Spiders that inherit from the project&rsquo;s base spider classes are expected to return an object of keys to an </span><span class="c14">array</span><span>&nbsp;of values. Each array </span><span>must </span><span class="c6">be the same length. All values at the same index are assumed to be for the same event. </span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span>The </span><span class="c7">parse() </span><span>method takes in a response object which represents the HTML returned by the request specified. Extracting data from this object is done exactly as you would extract data in any scrapy spider. In this tutorial, the </span><span class="c7">.css()</span><span>&nbsp;method (</span><span class="c3"><a class="c1" href="https://www.google.com/url?q=https://docs.scrapy.org/en/latest/topics/selectors.html%23using-selectors&amp;sa=D&amp;ust=1570341218912000">https://docs.scrapy.org/en/latest/topics/selectors.html#using-selectors</a></span><span class="c6">) can be used to extract the data.</span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c6">If we press F12 (or whatever shortcut opens the developer/inspect console in your browser) we can see that the data we need is neatly organized in a table. What looks like the title and url is in the 3rd column, the date is in the 1st column, and something that could work as the description is in the 4th column. </span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 629.56px; height: 244.50px;"><img alt="" src="images/image1.png" style="width: 788.85px; height: 444.66px; margin-left: -97.34px; margin-top: -115.28px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span>Taking advantage of the css </span><span class="c7">:nth-child(n)</span><span>&nbsp;selector (</span><span class="c3"><a class="c1" href="https://www.google.com/url?q=https://developer.mozilla.org/en-US/docs/Web/CSS/:nth-child&amp;sa=D&amp;ust=1570341218913000">https://developer.mozilla.org/en-US/docs/Web/CSS/:nth-child</a></span><span>), the following code could be used to extract the data. (</span><span class="c14">Authors note: this code is intentionally incorrect to demonstrate the usefulness of a utility function introduced later</span><span class="c6">).</span></p><p class="c5"><span class="c6"></span></p><a id="t.8dce2d9b4eeb322af397979719c5c7ea9cc73111"></a><a id="t.2"></a><table class="c15"><tbody><tr class="c17"><td class="c13" colspan="1" rowspan="1"><p class="c11"><span class="c0 c4">def</span><span class="c9 c2">&nbsp;parse(self, response):</span></p><p class="c11"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0 c4">return</span><span class="c9 c2">&nbsp;{</span></p><p class="c11"><span class="c9 c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&#39;title&#39;: response.css(&#39;table tr td:nth-child(3) span::text&#39;).extract(),</span></p><p class="c11"><span class="c9 c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&#39;url&#39;: response.css(&#39;table tr td:nth-child(3) span::attr(href)&#39;).extract(),</span></p><p class="c11"><span class="c9 c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&#39;event_time&#39;: self.create_time_data(</span></p><p class="c11"><span class="c9 c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; date=response.css(&#39;table tr td:nth-child(3) span::text&#39;).extract()</span></p><p class="c11"><span class="c9 c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;),</span></p><p class="c11"><span class="c9 c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&#39;description&#39;: response.css(&#39;table tr td:nth-child(4)::text&#39;).extract()</span></p><p class="c11"><span class="c9 c2">&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p></td></tr></tbody></table><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c7">::text</span><span>&nbsp;will gets only the text from the retrieved elements.</span><span class="c7">&nbsp;::attr()</span><span>&nbsp;gets the value of the given attribute on the retrieved elements. The </span><span class="c7">extract() </span><span>function will get all elements that match those selectors </span><span class="c3"><a class="c1" href="https://www.google.com/url?q=https://docs.scrapy.org/en/latest/topics/selectors.html%23extract-and-extract-first&amp;sa=D&amp;ust=1570341218916000">https://docs.scrapy.org/en/latest/topics/selectors.html#extract-and-extract-first</a></span><span class="c6">. Both of these elements return arrays of strings where each string is one of the matches found. </span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span>The </span><span class="c0">event_time</span><span>&nbsp;field is special in that it is supposed to be an object where each field of the object is an array of strings. The utility function </span><span class="c7">create_time_data()</span><span>&nbsp;(TODO: add link) should always be used for this field. We pass in the </span><span class="c0">date</span><span class="c6">&nbsp;parameter since the Chi Hack Night events page only provides a date for the event. </span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c9 c12">Testing the Scraper</span></p><p class="c8"><span>To run only a specific scraper, you can run the </span><span class="c7">./start.sh</span><span>&nbsp;command with the </span><span class="c7">--spider-name </span><span class="c6">parameter to specify what spider you want to run - this is perfect for testing a spider. </span></p><p class="c5"><span class="c6"></span></p><a id="t.99288a33bfef0b7c34fc5393ccc620541406c0ce"></a><a id="t.3"></a><table class="c15"><tbody><tr class="c17"><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c9 c2">sudo ./start.sh --spider-name chihacknight</span></p></td></tr></tbody></table><p class="c5"><span class="c6"></span></p><p class="c8"><span>(</span><span class="c14">Authors note: the command above is for a Linux machine. If you have a different operating system it might look a little different for you</span><span>). However, although the selectors are correct and the logic is sound, the </span><span class="c7">event_processor</span><span class="c6">&nbsp;container gets this error:</span></p><p class="c5"><span class="c6"></span></p><a id="t.3d4905e7ad9328cfe8a5743bef9377a4730a3f0f"></a><a id="t.4"></a><table class="c15"><tbody><tr class="c17"><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c9 c2">...</span></p><p class="c8"><span class="c2 c9">event_processor_1 &nbsp;| &nbsp; File &quot;/usr/src/app/event_processor/scrapy_impl/middlewares.py&quot;, line 18, in get_event_count</span></p><p class="c8"><span class="c9 c2">event_processor_1 &nbsp;| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise ValueError(f&#39;{spider.organization}: Selectors returned data of differing lengths&#39;)</span></p><p class="c8"><span class="c2">event_processor_1 &nbsp;| </span><span class="c9 c0 c4">ValueError: Chi Hack Night: Selectors returned data of differing lengths</span></p><p class="c8"><span class="c9 c2">event_processor_1 &nbsp;| 2019-10-01 01:44:28,840 - chihacknight - INFO - No data returned for https://chihacknight.org/</span></p><p class="c8"><span class="c9 c2">event_processor_1 &nbsp;| Event processor completed</span></p><p class="c8"><span class="c9 c2">...</span></p></td></tr></tbody></table><p class="c5"><span class="c6"></span></p><p class="c8"><span>So what happened? The event processor expects each field in the object returned by the scraper to be an array of values such that each array is the same length. This is because it needs to correctly associate each piece data to each event, which is can only do reliable if each array is the same length. Because </span><span class="c7">extract() </span><span class="c6">does not return data it can&rsquo;t find, some of the arrays might have been different lengths, resulting in an error. </span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c9 c12">Fixing the Scraper</span></p><p class="c8"><span>The ScraperSpider base class has a function </span><span class="c7">empty_check_extract()</span><span>&nbsp;(TODO documentation link) which will attempt to find some data given a selector to some section of HTML and then some data within that section of HTML. If it doesn&rsquo;t find data using the second selector, then it will replace it with an empty string. This way, as long as the first selector of each call of this function returns the same number of elements, the array returned by this function will always be the same length. We can replace each field with a </span><span class="c7">empty_check_extract()</span><span class="c6">&nbsp;call.</span></p><p class="c5"><span class="c6"></span></p><a id="t.d6cb77d17f9da8052a31284d0f9c53b123528768"></a><a id="t.5"></a><table class="c15"><tbody><tr class="c17"><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c0 c4">def</span><span class="c9 c2">&nbsp;parse(self, response):</span></p><p class="c8"><span class="c2">&nbsp; &nbsp; </span><span class="c0 c4">return</span><span class="c9 c2">&nbsp;{</span></p><p class="c8"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &#39;title&#39;: </span><span class="c0 c4">self</span><span class="c2">.</span><span class="c0 c4">empty_check_extract</span><span class="c9 c2">(response.css(&#39;table tr&#39;), self.css_func, </span></p><p class="c8"><span class="c9 c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &#39;td:nth-child(3) span::text&#39;),</span></p><p class="c8"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &#39;url&#39;: </span><span class="c0 c4">self</span><span class="c2">.</span><span class="c0 c4">empty_check_extract</span><span class="c9 c2">(response.css(&#39;table tr&#39;), self.css_func, &nbsp;</span></p><p class="c8"><span class="c9 c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &#39;td:nth-child(3) a::attr(href)&#39;),</span></p><p class="c8"><span class="c9 c2">&nbsp; &nbsp; &nbsp; &nbsp; &#39;event_time&#39;: self.create_time_data(</span></p><p class="c8"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;date=</span><span class="c0 c4">self</span><span class="c2">.</span><span class="c0 c4">empty_check_extract</span><span class="c9 c2">(response.css(&#39;table tr&#39;), self.css_func, </span></p><p class="c8"><span class="c9 c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &#39;td:nth-child(1) p::text&#39;, &#39;Jan 01, 2012&#39;)</span></p><p class="c8"><span class="c9 c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span></p><p class="c8"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &#39;address&#39;: list(map(</span><span class="c0 c4">lambda</span><span class="c9 c2">&nbsp;x: &#39;222 Merchandise Mart Plaza, Chicago, IL 60654&#39;, &nbsp; </span></p><p class="c8"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0 c4">self</span><span class="c2">.</span><span class="c0 c4">empty_check_extract</span><span class="c9 c2">(response.css(&#39;table tr&#39;), self.css_func, </span></p><p class="c8"><span class="c9 c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &#39;td::text&#39;))),</span></p><p class="c8"><span class="c2">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#39;description&#39;: self.</span><span class="c0 c4">empty_check_extract</span><span class="c9 c2">(response.css(&#39;table tr&#39;), self.css_func, &#39;td:nth-child(4)::text&#39;)</span></p><p class="c8"><span class="c9 c2">&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p><p class="c5"><span class="c9 c2"></span></p></td></tr></tbody></table><p class="c5"><span class="c6"></span></p><p class="c8"><span>Additionally, I added a value for &ldquo;address&rdquo; which uses a lambda function to transform each returned result to the address of the Merchandise Mart. Since </span><span class="c7">empty_check_extract()</span><span class="c6">&nbsp;will make the array the same length as every other field and each element is transformed to be the same, this is a valid value for the address field. If we rerun the script, we should see something like this...</span></p><p class="c5"><span class="c6"></span></p><a id="t.4c8c22dbe7ef274261031a44346b76b7ac6237e4"></a><a id="t.6"></a><table class="c15"><tbody><tr class="c17"><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c9 c2">...</span></p><p class="c8"><span class="c9 c2">event_processor_1 &nbsp;| Running event processor...</span></p><p class="c8"><span class="c9 c2">event_processor_1 &nbsp;| 2019-10-01 01:56:00,762 - chihacknight - INFO - Found 7 events for Chi Hack Night.</span></p><p class="c8"><span class="c2">event_processor_1 &nbsp;| </span><span class="c9 c0 c4">2019-10-01 01:56:00,815 - chihacknight - INFO - Saved 7 events for Chi Hack Night</span></p><p class="c8"><span class="c9 c2">event_processor_1 &nbsp;| Event processor completed</span></p><p class="c8"><span class="c9 c2">event_processor_1 &nbsp;| Data retrieved successfully</span></p><p class="c8"><span class="c9 c2">...</span></p></td></tr></tbody></table><p class="c5"><span class="c6"></span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c6">The scraper will only save events which occur in the future based on the event date and time. Checking the database&rsquo;s event table, we can find our events to confirm it worked as expected (</span></p><p class="c5"><span class="c6"></span></p><p class="c5"><span class="c6"></span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c9 c21">Crawling Web Scraper</span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c6">(Authors note: not yet written &hellip; or nicely formatted)</span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span>The ScraperCrawlSpider inherits from scrapy&rsquo;s CrawlSpider </span><span class="c3"><a class="c1" href="https://www.google.com/url?q=https://docs.scrapy.org/en/latest/topics/spiders.html%23crawlspider&amp;sa=D&amp;ust=1570341218929000">https://docs.scrapy.org/en/latest/topics/spiders.html#crawlspider</a></span><span class="c6">&nbsp;</span></p><ul class="c16 lst-kix_ryua6xjiznqm-0 start"><li class="c8 c10"><span class="c6">Define set of rules for following links (rules attribute) </span></li></ul><ul class="c16 lst-kix_ryua6xjiznqm-1 start"><li class="c8 c24"><span>First argument, and only required one, is a Link Extractor </span><span class="c3"><a class="c1" href="https://www.google.com/url?q=https://docs.scrapy.org/en/latest/topics/link-extractors.html%23topics-link-extractors&amp;sa=D&amp;ust=1570341218930000">https://docs.scrapy.org/en/latest/topics/link-extractors.html#topics-link-extractors</a></span><span class="c6">&nbsp;</span></li></ul><p class="c5"><span class="c6"></span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c9 c12">Define the Scraper</span></p><p class="c8"><span>Just like before, create a new scraper file in </span><span class="c7">event_processor/scrapers</span><span>&nbsp;called </span><span class="c0">chihacknight_crawl_spider.py</span><span class="c6">. Import ScraperCrawlSpider for the base class and include the UTF-8 encoding comment on top. </span></p><p class="c5"><span class="c6"></span></p><a id="t.92a7574747225d6766171bfd77d4e1ad11261e98"></a><a id="t.7"></a><table class="c15"><tbody><tr class="c17"><td class="c13" colspan="1" rowspan="1"><p class="c8"><span class="c9 c2"># -*- coding: utf-8 -*-</span></p><p class="c8"><span class="c0 c4">from</span><span class="c2">&nbsp;base.custom_spiders </span><span class="c0 c4">import</span><span class="c9 c2">&nbsp;ScraperCrawlSpider</span></p><p class="c5"><span class="c9 c2"></span></p><p class="c8"><span class="c0 c4">class</span><span class="c9 c2">&nbsp;ChiHackNightCrawlSpider(ScraperCrawlSpider):</span></p><p class="c8"><span class="c9 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name = &#39;chihacknightcrawl&#39;</span></p><p class="c8"><span class="c9 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;allowed_domains = [&#39;chihacknight.org&#39;]</span></p><p class="c8"><span class="c9 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enabled = True</span></p><p class="c5"><span class="c9 c2"></span></p><p class="c8"><span class="c9 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rules = (</span></p><p class="c8"><span class="c9 c2">&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; Rule(LinkExtractor(restrict_css = &#39;.title&#39;))</span></p><p class="c8"><span class="c9 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)</span></p></td></tr></tbody></table><p class="c5"><span class="c6"></span></p><p class="c8"><span>Just like before, we define a distinct </span><span class="c0">name</span><span>, </span><span class="c0">allowed_domains</span><span>&nbsp;for our site, and </span><span class="c0">enabled</span><span>&nbsp;to mark that this spider is enabled. However, a crawl spider also needs to know what links it is allowed to crawl so that it doesn&rsquo;t go on a crawling rampage. We also define a </span><span class="c0">rules</span><span>&nbsp;attribute which is a </span><span class="c14">list</span><span>&nbsp;of scrapy Crawling Rules (</span><span class="c3"><a class="c1" href="https://www.google.com/url?q=https://docs.scrapy.org/en/latest/topics/spiders.html%23crawling-rules&amp;sa=D&amp;ust=1570341218934000">https://docs.scrapy.org/en/latest/topics/spiders.html#crawling-rules</a></span><span class="c6">). </span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span>The first argument of the Rule constructor is a Link Extractor (</span><span class="c3"><a class="c1" href="https://www.google.com/url?q=https://docs.scrapy.org/en/latest/topics/link-extractors.html%23topics-link-extractors&amp;sa=D&amp;ust=1570341218935000">https://docs.scrapy.org/en/latest/topics/link-extractors.html#topics-link-extractor</a></span><span class="c6">)</span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c9 c12">Initialize the Scraper</span></p><p class="c8"><span class="c6">Initiailize the scraper</span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c6">&nbsp;</span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c22">Parse the Results</span></p><p class="c8"><span class="c6">Parse the results</span></p><p class="c5"><span class="c6"></span></p><p class="c8"><span class="c9 c12">Testing the Scraper</span></p><p class="c8"><span>Testing the scraper</span></p></body></html>