import scrapy
from urllib import parse
from aggregator_base import AggregatorBase
# https://doc.scrapy.org/en/latest/topics/loaders.html
class SpiderBase(AggregatorBase):
    # This class includes all functionality that should be shared by spiders

    def xpath_func(self, data):
        return data.xpath

    def css_func(self, data):
        return data.css
    
    def get_request(self, url, request_params={}):
        return scrapy.Request(f'{self.base_url}{url}?{parse.urlencode(request_params)}')

    # def extract(self, name, extractor, path):
    #     # Remove leading and trailing whitespace from all extracted values
    #     return EventFieldData(name, list(map(lambda s: s.strip(), extractor(path).extract())))

    # def re_extract(self, name, extractor, path, pattern):
    #     """
    #     Use this method to apply a regex filter to the extracted data

    #     :param name: The name of the event property extracted by this selector\n
    #     :param extractor: either response.css or response.xpath\n
    #     :param path: css or xpath selector string\n
    #     :param pattern: regex string to extract\n
    #     :returns: new EventFieldData object with the extracted data
    #     """
    #     return EventFieldData(name, list(map(lambda s: s.strip(), extractor(path).re(pattern))))

    def extract_multiple(self, name_funcs, extracted_data):
        return_data = dict()
        for name, name_func in name_funcs.values():
            return_data[name] = [name_func(data) for data in extracted_data]
        return return_data

    # def extract_multiple(self, name_funcs, extractor, path):
    #     """
    #     This method is necessary when there are multiple fields contained within a single xpath selector (e.g. a date and a time).
    #     name_funcs should be a dictionary in which the keys are event property names and the values are functions that extract
    #     the value that matches that key.

    #     :param name_funcs: A dictionary whose keys are the names of event properties and values are functions which take in a single string (the extracted value)
    #     and return the portion of the extracted value which represents the event property named in the key\n
    #     :param extractor: either response.css or response.xpath\n
    #     param path: css or xpath selector string\n
    #     :returns: a generator yielding each EventFieldData object with the extracted data
    #     """
    #     for name in name_funcs.keys():
    #         yield EventFieldData(name, list(map(lambda s: name_funcs[name](s).strip(), extractor(path).extract())))
    
    # def empty_check_extract2(self, name, base_selector, extractor_name, path, default_value=''):
    #     """
    #     Search for all values that match the xpath or css selector within the base selector and add a default value if nothing is found.
    #     Scrapy's selectors don't add anything to the response array if no value is found, so this
    #     method is necessary for semi-structured html blocks where a field could be missing.

    #     :param name: The name of the event property extracted by this selector\n
    #     :param base_selector: result of response.css or response.xpath call that contains the data to be processed further\n
    #     :param extractor_name: 'css' or 'xpath' depending on if the path variable is a css or xpath string\n
    #     :param path: css or xpath selector string\n
    #     :param default_value: What to return when the value isn't found in the selector\n
    #     :returns: new EventFieldData object with the extracted data
    #     """
    #     data = []
    #     for base_data in base_selector:
    #         extracted_data = eval(f'base_data.{extractor_name}(path).extract()')
    #         # Add a placeholder value if nothing was found on the site
    #         if len(extracted_data) == 0:
    #             extracted_data = [default_value]
    #         data.extend(extracted_data)
        
    #     return EventFieldData(name, data)

    def empty_check_extract(self, base_selector, extractor, path, default_value=''):
        return list(map(lambda data: default_value if len(data) == 0 else data[0], [extractor(base_data)(path).extract() for base_data in base_selector]))

    def create_time_data(self, **kwargs):
        count = len(list(kwargs.values())[0])
        for value in kwargs.values():
            if len(value) != count:
                raise ValueError(f'{self.organization}: Time selectors returned data of differing lengths')
        return [{key: value[i] for key, value in kwargs.items()} for i in range(count)]
        

    # def create_events(self, organization, *args):
    #     """
    #     Call this function once all of the data from the site has been extracted into variables

    #     :param organization: Name of the organization that owns the site\n
    #     :param *args: All EventFieldData objects generated by the extract methods in this class
    #     :returns: Generator yielding Event objects created from the EventFieldData objects
    #     """
    #     count = len(args[0].data)
    #     for arg in args:
    #         if len(arg.data) != count:
    #             # All selectors must return the same amount of data because it's impossible to know which event is missing data otherwise
    #             raise ValueError(f'{organization}: Selectors returned data of differing lengths')

    #     events = (Event.from_dict({arg.item: arg.data[i] for arg in args}, self.time_utils.date_format) for i in range(count))
    #     for event in events:
    #         # Only return events that are in the date range that we care about
    #         if self.time_utils.time_range_is_between(event['start_timestamp'], event['end_timestamp'], self.start_timestamp, self.end_timestamp):
    #             event['organization'] = organization
    #             yield event
            

