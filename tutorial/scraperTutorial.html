<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=cGvuclDC_Z1vE_cnVEU6Ae_NZQ7StBcqH_vXVqoPMX0');.lst-kix_egkpyywq84m4-4>li:before{content:"\0025cb  "}.lst-kix_egkpyywq84m4-3>li:before{content:"\0025cf  "}.lst-kix_egkpyywq84m4-5>li:before{content:"\0025a0  "}.lst-kix_8fuq8mrjccaa-0>li:before{content:"\0025cf  "}.lst-kix_8fuq8mrjccaa-2>li:before{content:"\0025a0  "}.lst-kix_egkpyywq84m4-2>li:before{content:"\0025a0  "}.lst-kix_egkpyywq84m4-6>li:before{content:"\0025cf  "}.lst-kix_avobg7ugvjvy-0>li:before{content:"\0025cf  "}.lst-kix_8fuq8mrjccaa-3>li:before{content:"\0025cf  "}.lst-kix_avobg7ugvjvy-1>li:before{content:"\0025cb  "}.lst-kix_avobg7ugvjvy-2>li:before{content:"\0025a0  "}.lst-kix_ryua6xjiznqm-8>li:before{content:"-  "}.lst-kix_8fuq8mrjccaa-1>li:before{content:"\0025cb  "}.lst-kix_ryua6xjiznqm-5>li:before{content:"-  "}ul.lst-kix_avobg7ugvjvy-6{list-style-type:none}ul.lst-kix_6xyoogcg4v-8{list-style-type:none}ul.lst-kix_avobg7ugvjvy-7{list-style-type:none}ul.lst-kix_6xyoogcg4v-7{list-style-type:none}ul.lst-kix_avobg7ugvjvy-4{list-style-type:none}ul.lst-kix_avobg7ugvjvy-5{list-style-type:none}.lst-kix_8fuq8mrjccaa-8>li:before{content:"\0025a0  "}.lst-kix_ryua6xjiznqm-3>li:before{content:"-  "}.lst-kix_ryua6xjiznqm-7>li:before{content:"-  "}ul.lst-kix_avobg7ugvjvy-2{list-style-type:none}ul.lst-kix_6xyoogcg4v-4{list-style-type:none}ul.lst-kix_avobg7ugvjvy-3{list-style-type:none}ul.lst-kix_6xyoogcg4v-3{list-style-type:none}.lst-kix_ryua6xjiznqm-2>li:before{content:"-  "}.lst-kix_ryua6xjiznqm-6>li:before{content:"-  "}ul.lst-kix_avobg7ugvjvy-0{list-style-type:none}ul.lst-kix_6xyoogcg4v-6{list-style-type:none}ul.lst-kix_avobg7ugvjvy-1{list-style-type:none}.lst-kix_8fuq8mrjccaa-7>li:before{content:"\0025cb  "}ul.lst-kix_6xyoogcg4v-5{list-style-type:none}.lst-kix_egkpyywq84m4-0>li:before{content:"\0025cf  "}ul.lst-kix_6xyoogcg4v-0{list-style-type:none}.lst-kix_egkpyywq84m4-1>li:before{content:"\0025cb  "}ul.lst-kix_6xyoogcg4v-2{list-style-type:none}.lst-kix_8fuq8mrjccaa-4>li:before{content:"\0025cb  "}.lst-kix_8fuq8mrjccaa-6>li:before{content:"\0025cf  "}ul.lst-kix_6xyoogcg4v-1{list-style-type:none}.lst-kix_ryua6xjiznqm-4>li:before{content:"-  "}ul.lst-kix_avobg7ugvjvy-8{list-style-type:none}.lst-kix_8fuq8mrjccaa-5>li:before{content:"\0025a0  "}ul.lst-kix_8fuq8mrjccaa-5{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-4{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-3{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-2{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-8{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-7{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-6{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-1{list-style-type:none}ul.lst-kix_8fuq8mrjccaa-0{list-style-type:none}ul.lst-kix_ryua6xjiznqm-6{list-style-type:none}.lst-kix_6xyoogcg4v-3>li:before{content:"\0025cf  "}ul.lst-kix_ryua6xjiznqm-5{list-style-type:none}ul.lst-kix_ryua6xjiznqm-8{list-style-type:none}ul.lst-kix_ryua6xjiznqm-7{list-style-type:none}.lst-kix_6xyoogcg4v-1>li:before{content:"\0025cb  "}.lst-kix_6xyoogcg4v-5>li:before{content:"\0025a0  "}.lst-kix_6xyoogcg4v-0>li:before{content:"\0025cf  "}.lst-kix_6xyoogcg4v-4>li:before{content:"\0025cb  "}.lst-kix_egkpyywq84m4-8>li:before{content:"\0025a0  "}.lst-kix_egkpyywq84m4-7>li:before{content:"\0025cb  "}ul.lst-kix_ryua6xjiznqm-0{list-style-type:none}ul.lst-kix_ryua6xjiznqm-2{list-style-type:none}ul.lst-kix_ryua6xjiznqm-1{list-style-type:none}ul.lst-kix_ryua6xjiznqm-4{list-style-type:none}.lst-kix_6xyoogcg4v-2>li:before{content:"\0025a0  "}ul.lst-kix_ryua6xjiznqm-3{list-style-type:none}ul.lst-kix_egkpyywq84m4-0{list-style-type:none}ul.lst-kix_egkpyywq84m4-1{list-style-type:none}ul.lst-kix_egkpyywq84m4-2{list-style-type:none}ul.lst-kix_egkpyywq84m4-3{list-style-type:none}.lst-kix_6xyoogcg4v-8>li:before{content:"\0025a0  "}ul.lst-kix_egkpyywq84m4-4{list-style-type:none}ul.lst-kix_egkpyywq84m4-5{list-style-type:none}.lst-kix_6xyoogcg4v-7>li:before{content:"\0025cb  "}ul.lst-kix_egkpyywq84m4-6{list-style-type:none}ul.lst-kix_egkpyywq84m4-7{list-style-type:none}ul.lst-kix_egkpyywq84m4-8{list-style-type:none}.lst-kix_6xyoogcg4v-6>li:before{content:"\0025cf  "}.lst-kix_ryua6xjiznqm-1>li:before{content:"-  "}.lst-kix_avobg7ugvjvy-8>li:before{content:"\0025a0  "}.lst-kix_ryua6xjiznqm-0>li:before{content:"-  "}.lst-kix_avobg7ugvjvy-4>li:before{content:"\0025cb  "}.lst-kix_avobg7ugvjvy-5>li:before{content:"\0025a0  "}.lst-kix_avobg7ugvjvy-3>li:before{content:"\0025cf  "}.lst-kix_avobg7ugvjvy-7>li:before{content:"\0025cb  "}.lst-kix_avobg7ugvjvy-6>li:before{content:"\0025cf  "}ol{margin:0;padding:0}table td,table th{padding:0}.c18{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:468pt;border-top-color:#000000;border-bottom-style:solid}.c6{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:504.8pt;border-top-color:#000000;border-bottom-style:solid}.c2{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c25{margin-left:36pt;padding-top:0pt;text-indent:36pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:right}.c23{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c17{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c8{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c24{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c5{color:#000000;text-decoration:none;vertical-align:baseline;font-size:10pt;font-style:normal}.c21{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c16{border-spacing:0;border-collapse:collapse;margin-right:auto}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c14{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c1{font-size:10pt;font-family:"Consolas";font-weight:700}.c20{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c10{font-weight:700;font-family:"Consolas"}.c15{color:inherit;text-decoration:inherit}.c22{padding:0;margin:0}.c4{font-weight:400;font-family:"Consolas"}.c19{font-size:12pt;font-weight:700}.c13{font-size:10pt}.c12{height:11pt}.c11{font-style:italic}.c26{font-weight:700}.c9{height:0pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c20"><div><p class="c25"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Edited 10/20/19</span></p></div><p class="c3"><span class="c17">Chi Hack Night Scraper Tutorial</span></p><p class="c3"><span>In this tutorial, we will be creating a 2 web scrapers for the Chi Hack Night events page </span><span class="c14"><a class="c15" href="https://www.google.com/url?q=https://chihacknight.org/events/index.html&amp;sa=D&amp;ust=1571612733103000">https://chihacknight.org/events/index.html</a></span><span class="c0">. The first one will be simple and only extract data from the event listing page. The second one will be more complex and extract more complete data from each event page.</span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>If you want to reference the completed scrapers, the repository already contains both web scrapers you will create in this tutorial (</span><span class="c11">Authors note: not yet actually (TODO)</span><span class="c0">)</span></p><p class="c3"><span class="c21 c4">event_processor/scrapers/chihacknight_simple_spider.py</span></p><p class="c3"><span class="c4 c21">event_processor/scrapers/chihacknight_crawling_spider.py</span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c17">Prerequisites</span></p><p class="c3"><span class="c0">You should be comfortable with: </span></p><ul class="c22 lst-kix_8fuq8mrjccaa-0 start"><li class="c2"><span class="c0">HTML/DOM</span></li><li class="c2"><span class="c0">CSS Selectors</span></li><li class="c2"><span class="c0">Python</span></li></ul><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c0">This tutorial assumes you are using a Linux machine, so if you&rsquo;re using something else the shell commands would be a little different. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c17">Web Scrapers in In2It</span></p><p class="c3"><span class="c0">Web scrapers are important for the the In2It Chicago project because they extract all event data for the site in a relatively normalized and searchable format. While users could always use a traditional search engine to find these events, In2It Chicago is meant to focus on events related to community involvement and support. Each web scraper is tailor made for each site.</span></p><p class="c3"><span>The scrapers are python classes which inherit classes from scrapy </span><span class="c14"><a class="c15" href="https://www.google.com/url?q=https://scrapy.org/&amp;sa=D&amp;ust=1571612733105000">https://scrapy.org/</a></span><span class="c0">, which is the most popular python web scraping library available.</span></p><p class="c3"><span class="c0">Other than web scrapers, the site also extracts data from APIs (and what else)? </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c17">Project file structure</span></p><p class="c3"><span>The web scrapers all live in the </span><span class="c4">event_processor</span><span class="c0">&nbsp;container- it is just one of the many In2It Chicago docker containers. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>The subfolder </span><span class="c4">event_processor/event_processor</span><span class="c0">&nbsp;is a python package containing all relevant files. The important subfolders are as follows:</span></p><ul class="c22 lst-kix_avobg7ugvjvy-0 start"><li class="c2"><span class="c4">base</span><span class="c0">&nbsp;- All base classes for custom data extractors</span></li><li class="c2"><span class="c4">scrapers</span><span class="c0">&nbsp;- Custom web scrapers</span></li><li class="c2"><span class="c4">apis</span><span class="c0">&nbsp;- Classes extracting data from public APIs </span></li><li class="c2"><span class="c4">scrapy_impl</span><span class="c0">&nbsp;- Scrapy middleware </span></li><li class="c2"><span class="c4">models</span><span class="c0">&nbsp;- Classes and utility functions for data container classes</span></li><li class="c2"><span class="c4">utils</span><span class="c0">&nbsp;- Utility functions</span></li></ul><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>Obviously, the folder we care about most in this tutorial is the </span><span class="c4">scrapers</span><span class="c0">&nbsp;folder. The first section will be for a simple scraper that only scrapes one page. The second section is a scraper that crawls and scrapes many pages. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c17">Simple Web Scraper</span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>The more simple ScraperSpider inherits from scrapy&rsquo;s ScrapySpider </span><span class="c14"><a class="c15" href="https://www.google.com/url?q=https://docs.scrapy.org/en/latest/topics/spiders.html%23scrapy-spider&amp;sa=D&amp;ust=1571612733107000">https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy-spider</a></span><span class="c0">&nbsp;and includes other utility methods specific to this project. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c8">Define the Scraper</span></p><p class="c3"><span>Go ahead and create a new custom scraper file in </span><span class="c4">event_processor/scrapers</span><span>&nbsp;called </span><span class="c10">chihacknight_spider.py</span><span class="c0">.</span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>Import the necessary base class for the simple scraper, ScraperSpider and define the spider name. The </span><span class="c4">coding: utf-8</span><span>&nbsp;comment on top defines the character encoding of the file. (</span><span class="c11">Authors note: I don&rsquo;t know if that is what it does or if it is even needed in python</span><span class="c0">) </span></p><p class="c3 c12"><span class="c0"></span></p><a id="t.7640930352fb9d44052614f0946ddc944ff8ceef"></a><a id="t.0"></a><table class="c16"><tbody><tr class="c9"><td class="c18" colspan="1" rowspan="1"><p class="c7"><span class="c5 c4"># -*- coding: utf-8 -*-</span></p><p class="c7"><span class="c1">from</span><span class="c4 c13">&nbsp;event_processor.base.custom_spiders </span><span class="c1">import</span><span class="c5 c4">&nbsp;ScraperSpider</span></p><p class="c7 c12"><span class="c5 c4"></span></p><p class="c7"><span class="c1">class</span><span class="c5 c4">&nbsp;ChiHackNightSpider(ScraperSpider):</span></p><p class="c7"><span class="c5 c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name = &#39;chihacknight&#39;</span></p><p class="c7"><span class="c5 c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;allowed_domains = [&#39;chihacknight.org&#39;]</span></p><p class="c7"><span class="c4 c13">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enabled = True</span></p></td></tr></tbody></table><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>The </span><span class="c10">name</span><span class="c0">&nbsp;attribute is how the scraper will be identified when running this spider from the command line. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>The </span><span class="c10">allowed_domains</span><span>&nbsp;attribute specifies what domains this scraper will be able to visit while it is scraping (</span><span class="c11">Authors note: I don&rsquo;t know that for sure I&rsquo;m just assuming</span><span>). Because we want to scrape the Chi Hack Night website, we only specify the </span><span class="c4">&lsquo;chihacknight.org&rsquo;</span><span class="c0">&nbsp;domain. </span></p><p class="c3"><span>While the </span><span class="c10">enabled</span><span>&nbsp;attribute is not required, it is useful to know that you can set enabled to </span><span class="c4">False</span><span class="c0">&nbsp;to stop the event_processor from running or scheduling this scraper. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c8">Initialize the Scraper</span></p><p class="c3"><span>We have to specify the parameters for the scraper and tell it where it where exactly it will be scraping data. Define the the </span><span class="c4">__init__</span><span>&nbsp;and </span><span class="c4">start_requests</span><span class="c0">&nbsp;methods for these purposes.</span></p><p class="c3 c12"><span class="c0"></span></p><a id="t.109969e0d51672745ecb2d44c176631a6b5606a8"></a><a id="t.1"></a><table class="c16"><tbody><tr class="c9"><td class="c18" colspan="1" rowspan="1"><p class="c7"><span class="c1">def </span><span class="c4 c13">__init__(self, name=None, </span><span class="c1">**</span><span class="c5 c4">kwargs):</span></p><p class="c7"><span class="c4 c13">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">super</span><span class="c5 c4">().__init__(self, &#39;Chi Hack Night&#39;, &#39;https://chihacknight.org/&#39;, date_format=&#39;%b %d, %Y&#39;, **kwargs)</span></p><p class="c7"><span class="c1">def </span><span class="c5 c4">start_requests(self):</span></p><p class="c7"><span class="c4 c13">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c1">yield </span><span class="c5 c4">self.get_request(&#39;events/&#39;, {})</span></p></td></tr></tbody></table><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>The date_format is a required argument on __init__ and it is a string specifying the date format the scraper will be searching for. The string must follow the format expected by python&rsquo;s </span><span class="c4">datetime.strptime()</span><span>&nbsp;function. A full reference can be found at </span><span class="c14"><a class="c15" href="https://www.google.com/url?q=https://docs.python.org/3/library/datetime.html%23strftime-strptime-behavior&amp;sa=D&amp;ust=1571612733113000">https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior</a></span><span>. Manually examining the event format, it looks like </span><span class="c4">&lsquo;%b %d, %Y&rsquo;</span><span class="c0">&nbsp;</span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>The </span><span class="c4">start_requests()</span><span>&nbsp;method specifies the page which the requests will start from. For spiders that extend from the </span><span class="c4">ScraperSpider</span><span class="c0">&nbsp;base class. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c19">Parse the Results</span></p><p class="c3"><span>Spiders that inherit from the project&rsquo;s base spider classes are expected to return an object of keys to an </span><span class="c11">array</span><span>&nbsp;of values. Each array </span><span>must </span><span class="c0">be the same length. All values at the same index are assumed to be for the same event. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>The </span><span class="c4">parse() </span><span>method takes in a response object which represents the HTML returned by the request specified. Extracting data from this object is done exactly as you would extract data in any scrapy spider. In this tutorial, the </span><span class="c4">.css()</span><span>&nbsp;method (</span><span class="c14"><a class="c15" href="https://www.google.com/url?q=https://docs.scrapy.org/en/latest/topics/selectors.html%23using-selectors&amp;sa=D&amp;ust=1571612733115000">https://docs.scrapy.org/en/latest/topics/selectors.html#using-selectors</a></span><span class="c0">) can be used to extract the data.</span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c0">If we press F12 (or whatever shortcut opens the developer/inspect console in your browser) we can see that the data we need is neatly organized in a table. What looks like the title and url is in the 3rd column, the date is in the 1st column, and something that could work as the description is in the 4th column. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 629.56px; height: 244.50px;"><img alt="" src="images/image2.png" style="width: 788.85px; height: 444.66px; margin-left: -97.34px; margin-top: -115.28px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>Taking advantage of the css </span><span class="c10">:nth-child(n)</span><span>&nbsp;selector (</span><span class="c14"><a class="c15" href="https://www.google.com/url?q=https://developer.mozilla.org/en-US/docs/Web/CSS/:nth-child&amp;sa=D&amp;ust=1571612733117000">https://developer.mozilla.org/en-US/docs/Web/CSS/:nth-child</a></span><span>), the following code could be used to extract the data. (</span><span class="c11">Authors note: this code is intentionally incorrect to demonstrate the usefulness of a utility function introduced later</span><span class="c0">).</span></p><p class="c3 c12"><span class="c0"></span></p><a id="t.8dce2d9b4eeb322af397979719c5c7ea9cc73111"></a><a id="t.2"></a><table class="c16"><tbody><tr class="c9"><td class="c6" colspan="1" rowspan="1"><p class="c7"><span class="c1">def</span><span class="c5 c4">&nbsp;parse(self, response):</span></p><p class="c7"><span class="c4 c13">&nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c1">return</span><span class="c5 c4">&nbsp;{</span></p><p class="c7"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&#39;title&#39;: response.css(&#39;table tr td:nth-child(3) span::text&#39;).extract(),</span></p><p class="c7"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&#39;url&#39;: response.css(&#39;table tr td:nth-child(3) span::attr(href)&#39;).extract(),</span></p><p class="c7"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&#39;event_time&#39;: self.create_time_data(</span></p><p class="c7"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; date=response.css(&#39;table tr td:nth-child(3) span::text&#39;).extract()</span></p><p class="c7"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;),</span></p><p class="c7"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&#39;description&#39;: response.css(&#39;table tr td:nth-child(4)::text&#39;).extract()</span></p><p class="c7"><span class="c5 c4">&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p></td></tr></tbody></table><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c4">::text</span><span>&nbsp;will gets only the text from the retrieved elements.</span><span class="c4">&nbsp;</span><span class="c10">::attr()</span><span>&nbsp;gets the value of the given attribute on the retrieved elements. The </span><span class="c10">extract() </span><span>function will get all elements that match those selectors </span><span class="c14"><a class="c15" href="https://www.google.com/url?q=https://docs.scrapy.org/en/latest/topics/selectors.html%23extract-and-extract-first&amp;sa=D&amp;ust=1571612733121000">https://docs.scrapy.org/en/latest/topics/selectors.html#extract-and-extract-first</a></span><span class="c0">. Both of these elements return arrays of strings where each string is one of the matches found. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>The </span><span class="c10">event_time</span><span>&nbsp;field is special in that it is supposed to be an object where each field of the object is an array of strings. The utility function </span><span class="c10">create_time_data()</span><span class="c26">&nbsp;</span><span>should always be used for this field. We pass in the </span><span class="c10">date</span><span class="c0">&nbsp;parameter since the Chi Hack Night events page only provides a date for the event. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c8">Testing the Scraper</span></p><p class="c3"><span>To run only a specific scraper, you can run the </span><span class="c4">./start.sh</span><span>&nbsp;command with the </span><span class="c4">--spider-name </span><span class="c0">parameter to specify what spider you want to run - this is perfect for testing a spider. </span></p><p class="c3 c12"><span class="c0"></span></p><a id="t.99288a33bfef0b7c34fc5393ccc620541406c0ce"></a><a id="t.3"></a><table class="c16"><tbody><tr class="c9"><td class="c6" colspan="1" rowspan="1"><p class="c3"><span class="c5 c4">sudo ./start.sh --spider-name chihacknight</span></p></td></tr></tbody></table><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>(</span><span class="c11">Authors note: the command above is for a Linux machine. If you have a different operating system it might look a little different for you</span><span>). However, although the selectors are correct and the logic is sound, the </span><span class="c4">event_processor</span><span class="c0">&nbsp;container gets this error:</span></p><p class="c3 c12"><span class="c0"></span></p><a id="t.3d4905e7ad9328cfe8a5743bef9377a4730a3f0f"></a><a id="t.4"></a><table class="c16"><tbody><tr class="c9"><td class="c6" colspan="1" rowspan="1"><p class="c3"><span class="c5 c4">...</span></p><p class="c3"><span class="c5 c4">event_processor_1 &nbsp;| &nbsp; File &quot;/usr/src/app/event_processor/scrapy_impl/middlewares.py&quot;, line 18, in get_event_count</span></p><p class="c3"><span class="c5 c4">event_processor_1 &nbsp;| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise ValueError(f&#39;{spider.organization}: Selectors returned data of differing lengths&#39;)</span></p><p class="c3"><span class="c4 c13">event_processor_1 &nbsp;| </span><span class="c5 c10">ValueError: Chi Hack Night: Selectors returned data of differing lengths</span></p><p class="c3"><span class="c5 c4">event_processor_1 &nbsp;| 2019-10-01 01:44:28,840 - chihacknight - INFO - No data returned for https://chihacknight.org/</span></p><p class="c3"><span class="c4 c5">event_processor_1 &nbsp;| Event processor completed</span></p><p class="c3"><span class="c5 c4">...</span></p></td></tr></tbody></table><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>So what happened? The event processor expects each field in the object returned by the scraper to be an array of values such that each array is the same length. This is because it needs to correctly associate each piece data to each event, which is can only do reliable if each array is the same length. Because </span><span class="c4">extract() </span><span class="c0">does not return data it can&rsquo;t find, some of the arrays might have been different lengths, resulting in an error. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c8">Fixing the Scraper</span></p><p class="c3"><span>The ScraperSpider base class has a function </span><span class="c10">empty_check_extract()</span><span>&nbsp;which will attempt to find some data given a selector to some section of HTML and then some data within that section of HTML. If it doesn&rsquo;t find data using the second selector, then it will replace it with an empty string. This way, as long as the first selector of each call of this function returns the same number of elements, the array returned by this function will always be the same length. We can replace each field with a </span><span class="c10">empty_check_extract()</span><span class="c26">&nbsp;</span><span class="c0">call.</span></p><p class="c3 c12"><span class="c0"></span></p><a id="t.d6cb77d17f9da8052a31284d0f9c53b123528768"></a><a id="t.5"></a><table class="c16"><tbody><tr class="c9"><td class="c6" colspan="1" rowspan="1"><p class="c3"><span class="c1">def</span><span class="c5 c4">&nbsp;parse(self, response):</span></p><p class="c3"><span class="c4 c13">&nbsp; &nbsp; </span><span class="c1">return</span><span class="c5 c4">&nbsp;{</span></p><p class="c3"><span class="c4 c13">&nbsp; &nbsp; &nbsp; &nbsp; &#39;title&#39;: </span><span class="c1">self</span><span class="c4 c13">.</span><span class="c1">empty_check_extract</span><span class="c5 c4">(response.css(&#39;table tr&#39;), self.css_func, </span></p><p class="c3"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &#39;td:nth-child(3) span::text&#39;),</span></p><p class="c3"><span class="c4 c13">&nbsp; &nbsp; &nbsp; &nbsp; &#39;url&#39;: </span><span class="c1">self</span><span class="c4 c13">.</span><span class="c1">empty_check_extract</span><span class="c5 c4">(response.css(&#39;table tr&#39;), self.css_func, &nbsp;</span></p><p class="c3"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &#39;td:nth-child(3) a::attr(href)&#39;),</span></p><p class="c3"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &nbsp; &#39;event_time&#39;: self.create_time_data(</span></p><p class="c3"><span class="c4 c13">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;date=</span><span class="c1">self</span><span class="c4 c13">.</span><span class="c1">empty_check_extract</span><span class="c5 c4">(response.css(&#39;table tr&#39;), self.css_func, </span></p><p class="c3"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &#39;td:nth-child(1) p::text&#39;, &#39;Jan 01, 2012&#39;)</span></p><p class="c3"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;),</span></p><p class="c3"><span class="c4 c13">&nbsp; &nbsp; &nbsp; &nbsp; &#39;address&#39;: list(map(</span><span class="c1">lambda</span><span class="c5 c4">&nbsp;x: &#39;222 Merchandise Mart Plaza, Chicago, IL 60654&#39;, &nbsp; </span></p><p class="c3"><span class="c4 c13">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c1">self</span><span class="c4 c13">.</span><span class="c1">empty_check_extract</span><span class="c5 c4">(response.css(&#39;table tr&#39;), self.css_func, </span></p><p class="c3"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &#39;td::text&#39;))),</span></p><p class="c3"><span class="c4 c13">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#39;description&#39;: self.</span><span class="c1">empty_check_extract</span><span class="c5 c4">(response.css(&#39;table tr&#39;), self.css_func, &#39;td:nth-child(4)::text&#39;)</span></p><p class="c3"><span class="c5 c4">&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</span></p><p class="c3 c12"><span class="c5 c4"></span></p></td></tr></tbody></table><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>Additionally, I added a value for </span><span class="c4">address</span><span>&nbsp;which uses a lambda function to transform each returned result to the address of the Merchandise Mart. Since </span><span class="c4">empty_check_extract()</span><span class="c0">&nbsp;will make the array the same length as every other field and each element is transformed to be the same, this is a valid value for the address field. If we rerun the script, we should see something like this&hellip;</span></p><p class="c3 c12"><span class="c0"></span></p><a id="t.4c8c22dbe7ef274261031a44346b76b7ac6237e4"></a><a id="t.6"></a><table class="c16"><tbody><tr class="c9"><td class="c6" colspan="1" rowspan="1"><p class="c3"><span class="c5 c4">...</span></p><p class="c3"><span class="c5 c4">event_processor_1 &nbsp;| Running event processor...</span></p><p class="c3"><span class="c5 c4">event_processor_1 &nbsp;| 2019-10-01 01:56:00,762 - chihacknight - INFO - Found 7 events for Chi Hack Night.</span></p><p class="c3"><span class="c4 c13">event_processor_1 &nbsp;| </span><span class="c5 c10">2019-10-01 01:56:00,815 - chihacknight - INFO - Saved 7 events for Chi Hack Night</span></p><p class="c3"><span class="c5 c4">event_processor_1 &nbsp;| Event processor completed</span></p><p class="c3"><span class="c5 c4">event_processor_1 &nbsp;| Data retrieved successfully</span></p><p class="c3"><span class="c5 c4">...</span></p></td></tr></tbody></table><p class="c3 c12"><span class="c0"></span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c0">The scraper will only save events which occur in the future based on the event date and time. Checking the database&rsquo;s event table, we can find our events to confirm it worked as expected (</span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c17">Crawling Web Scraper</span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>The ScraperCrawlSpider inherits from scrapy&rsquo;s CrawlSpider </span><span class="c14"><a class="c15" href="https://www.google.com/url?q=https://docs.scrapy.org/en/latest/topics/spiders.html%23crawlspider&amp;sa=D&amp;ust=1571612733136000">https://docs.scrapy.org/en/latest/topics/spiders.html#crawlspider</a></span><span class="c0">&nbsp;and uses more of scrapy&rsquo;s built in features to crawl content from more than one page. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c8">Define the Scraper</span></p><p class="c3"><span>Just like before, create a new scraper file in </span><span class="c4">event_processor/scrapers</span><span>&nbsp;called </span><span class="c10">chihacknight_crawl_spider.py</span><span class="c0">. Import ScraperCrawlSpider for the base class and the scrapy classes Rule and LinkExtractor. Include the UTF-8 encoding comment on top. </span></p><p class="c3 c12"><span class="c0"></span></p><a id="t.76c19ebd7c75a712d8cf3d30a1b32c14cd9fc275"></a><a id="t.7"></a><table class="c16"><tbody><tr class="c9"><td class="c6" colspan="1" rowspan="1"><p class="c3"><span class="c5 c4"># -*- coding: utf-8 -*-</span></p><p class="c3"><span class="c1">from</span><span class="c4 c13">&nbsp;event_processor.base.custom_spiders </span><span class="c1">import</span><span class="c5 c4">&nbsp;ScraperCrawlSpider</span></p><p class="c3"><span class="c5 c4">from scrapy.spiders import Rule</span></p><p class="c3"><span class="c5 c4">from scrapy.linkextractors import LinkExtractor</span></p><p class="c3 c12"><span class="c5 c4"></span></p><p class="c3"><span class="c1">class</span><span class="c5 c4">&nbsp;ChiHackNightCrawlSpider(ScraperCrawlSpider):</span></p><p class="c3"><span class="c5 c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name = &#39;chihacknightcrawl&#39;</span></p><p class="c3"><span class="c5 c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;allowed_domains = [&#39;chihacknight.org&#39;]</span></p><p class="c3"><span class="c5 c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;start_urls = [&#39;https://chihacknight.org/events&#39;]</span></p><p class="c3"><span class="c5 c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enabled = True</span></p><p class="c3 c12"><span class="c5 c4"></span></p><p class="c3"><span class="c5 c4">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rules = (Rule(LinkExtractor(restrict_css = &#39;table tr td:nth-child(3) a&#39;), callback=&quot;parse_page&quot;, follow=True), )</span></p></td></tr></tbody></table><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>Just like before, we define a distinct </span><span class="c10">name</span><span>, </span><span class="c10">allowed_domains</span><span>&nbsp;for our site, and </span><span class="c10">enabled</span><span>&nbsp;to mark that this spider is enabled. However, a crawl spider also needs to know what links it is allowed to crawl so that it doesn&rsquo;t go on a crawling rampage. We also define a </span><span class="c10">rules</span><span>&nbsp;attribute which is a </span><span class="c11">list</span><span>&nbsp;of scrapy Crawling Rules (</span><span class="c14"><a class="c15" href="https://www.google.com/url?q=https://docs.scrapy.org/en/latest/topics/spiders.html%23crawling-rules&amp;sa=D&amp;ust=1571612733142000">https://docs.scrapy.org/en/latest/topics/spiders.html#crawling-rules</a></span><span class="c0">). </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>The first argument of the Rule constructor is a Link Extractor (</span><span class="c14"><a class="c15" href="https://www.google.com/url?q=https://docs.scrapy.org/en/latest/topics/link-extractors.html%23topics-link-extractors&amp;sa=D&amp;ust=1571612733143000">https://docs.scrapy.org/en/latest/topics/link-extractors.html#topics-link-extractor</a></span><span>), which takes in different parameters to filter links on the page. In this example, only the </span><span class="c10">restrict_css</span><span class="c0">&nbsp;parameter is needed since all links go to the same domain. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>The Crawling spider will crawl &nbsp;through pages itself, guided by the given rules. Rather than define a method for starting requests, we can instead define a list of URL strings </span><span class="c10">start_urls</span><span class="c0">&nbsp;which tells the crawler where to begin. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c8">Initialize the Scraper</span></p><p class="c3"><span class="c0">Initializing the spider is done a lot like the ScraperSpider crawler, only the date format is different to account for event pages using the full month name in the date.</span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>No </span><span class="c10">start_requests()</span><span>&nbsp;method is needed because the Crawler will look at the the </span><span class="c10">start_urls</span><span class="c0">&nbsp;list to know where to begin crawling.</span></p><p class="c3 c12"><span class="c0"></span></p><a id="t.5ccf68eebee9a6fbc50596e47780c7d06c4d34a0"></a><a id="t.8"></a><table class="c16"><tbody><tr class="c9"><td class="c6" colspan="1" rowspan="1"><p class="c3"><span class="c1">def </span><span class="c5 c4">__init__(self, name=None, **kwargs):</span></p><p class="c3"><span class="c1">&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super</span><span class="c4 c13">().__init__(self, &#39;Chi Hack Night&#39;, &#39;https://chihacknight.org&#39;, date_format=&#39;%B %d, %Y&#39;, **kwargs)</span></p></td></tr></tbody></table><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c19">Parse the Results</span></p><p class="c3"><span>The </span><span class="c4">ScraperCrawlSpider</span><span>&nbsp;will crawl to each link that applies to any of the rules defined in </span><span class="c4">rules</span><span>. If multiple rules apply, then it will crawl to the first one in the list. Parsing the results </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>Parsing data is done exactly like with the single page web scraper. Examining the HTML on a detail page for a Chi Hack Night event, it looks like most relevant information is nicely labeled with an &ldquo;itemprop&rdquo; attribute that we can use for the selector (</span><span class="c14"><a class="c15" href="https://www.google.com/url?q=https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors&amp;sa=D&amp;ust=1571612733147000">https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors</a></span><span class="c0">)</span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>The event page doesn&rsquo;t seem to contain its own URL on the page itself, so we instead get it from the </span><span class="c10">url</span><span>&nbsp;property passed in the </span><span class="c10">response</span><span class="c0">&nbsp;object.</span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>Some of the event pages seem to have no address, so we override the empty string default value in the </span><span class="c10">empty_check_extract()</span><span class="c0">&nbsp;method for the address field to handle those cases. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>For the address and description fields, the selectors end in </span><span class="c10">*::text</span><span class="c0">&nbsp;which means to get all text from all decendent elements returns by the given CSS selector. </span></p><p class="c3 c12"><span class="c0"></span></p><a id="t.0692a8bf49bde723729997666891563139b78c5f"></a><a id="t.9"></a><table class="c16"><tbody><tr class="c9"><td class="c6" colspan="1" rowspan="1"><p class="c3"><span class="c1">def</span><span class="c5 c4">&nbsp;parse_page(self, response):</span></p><p class="c3"><span class="c4 c13">&nbsp; &nbsp;</span><span class="c1">return</span><span class="c5 c4">&nbsp;{</span></p><p class="c3"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &#39;title&#39;: self.empty_check_extract(response.css(&#39;#primary-content&#39;), self.css_func, &#39; [itemprop=&quot;name&quot;]::text&#39;),</span></p><p class="c3"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &#39;url&#39;: list(map(lambda x: response.url, self.empty_check_extract( response.css(&#39;#primary-content&#39;), self.css_func, &#39;[itemprop=&quot;name&quot;]::text&#39;))),</span></p><p class="c3"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &#39;event_time&#39;: self.create_time_data(</span></p><p class="c3"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;date=self.empty_check_extract(response.css(&#39;#primary-content&#39;), self.css_func, &#39;[itemprop=&quot;startDate&quot;]::text&#39;)</span></p><p class="c3"><span class="c5 c4">&nbsp; &nbsp; &nbsp; ),</span></p><p class="c3"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &#39;address&#39;: self.empty_check_extract(response.css(&#39;#primary-content&#39;), self.css_func, &#39;[itemprop=&quot;address&quot;] *::text&#39;, default_value=&quot;222 Merchandise Mart Plaza, Chicago, IL 60654&quot;),</span></p><p class="c3"><span class="c5 c4">&nbsp; &nbsp; &nbsp; &#39;description&#39;: self.empty_check_extract(response.css(&#39;#primary-content&#39;), self.css_func, &#39;[itemprop=&quot;description&quot;] *::text&#39;)</span></p><p class="c3"><span class="c5 c4">&nbsp; &nbsp;}</span></p></td></tr></tbody></table><p class="c3 c12"><span class="c0"></span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c8">Testing the Scraper</span></p><p class="c3"><span class="c0">Just like before, run the crawl scraper by running ./start.sh with the given crawler&rsquo;s name.</span></p><p class="c3 c12"><span class="c0"></span></p><a id="t.3e17e609d46556457e553383cf2158e1642cea4c"></a><a id="t.10"></a><table class="c16"><tbody><tr class="c9"><td class="c6" colspan="1" rowspan="1"><p class="c3"><span class="c5 c4">sudo ./start.sh --spider-name chihacknightcrawl</span></p></td></tr></tbody></table><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c0">After some time, you should see log messages reporting that events for found for &ldquo;Chi Hack Night.&rdquo; </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span class="c19">Viewing the Events Table</span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>To see what the data looks like in the events database, Go to pgAdmin at </span><span class="c10">locahost:7000 </span><span>and sign in with username </span><span class="c10">user@domain.com</span><span>&nbsp;and password </span><span class="c10">pgadmin</span><span>. If you don&rsquo;t have the server added, right click on &ldquo;Servers&rdquo; on the left and select </span><span class="c10">Create &hellip;</span><span>&nbsp;and </span><span class="c10">Server&hellip;</span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span>You will be prompted to give information about the server you want to connect to. Give the server a meaningful name and switch to the </span><span class="c10">Connection</span><span>&nbsp;tab. The Port should be 5432. The Maintenance database should be postgres. Both the username and password is </span><span class="c10">postgres</span><span class="c0">. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c24"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 541.00px; height: 365.50px;"><img alt="" src="images/image1.png" style="width: 624.00px; height: 616.41px; margin-left: -38.00px; margin-top: -14.82px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c12 c24"><span class="c0"></span></p><p class="c3"><span>Once connected, navigate down the tree: </span><span class="c10">Databases</span><span>&nbsp;&gt; </span><span class="c10">events</span><span>&nbsp;&gt; </span><span class="c10">Schemas</span><span>&nbsp;&gt; </span><span class="c10">Tables</span><span>&nbsp;&gt; </span><span class="c10">events</span><span>. Right click on the events table and select </span><span class="c10">Scripts</span><span>&nbsp;&gt; </span><span class="c10">SELECT Script</span><span class="c0">. Execute the default SELECT script by clicking on the lightning bolt button to see what events are in the database. This is a good way to evaluate whether your scrapers are extracting the correct data. </span></p><p class="c3 c12"><span class="c0"></span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 592.00px; height: 281.00px;"><img alt="" src="images/image3.png" style="width: 1281.58px; height: 723.49px; margin-left: -308.07px; margin-top: -442.49px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></body></html>